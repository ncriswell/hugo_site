---
title: TLSTTS - Margin and Pythagorean Analysis
author: Nick Criswell
date: '2019-09-28'
slug: tlstts-margin-and-pythagorean-analysis
output:
  blogdown::html_page:
    toc: true
header-includes:
   - \usepackage{cancel}
   - \usepackage{[colorlinks = true, linkcolor = blue, urlcolor = blue, citecolor = blue, authorcolor = blue]{hyperref}}
categories:
  - fantasy
  - football
  - non-technical
  - R
tags: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.height = 5, 
                      fig.width = 7, 
                      fig.align= 'center')

library(jsonlite)       # Pulling down JSON data
library(tidyverse)      # Tools for data analysis
library(glue)           # Sticking strings together
library(RColorBrewer)   # Color Pals
library(plotly)         # Interactive graphics
library(DT)             # JS datatables package
library(ggthemes)       # For 538 theme
library(scales)         # Labels for ggplot scales
library(widgetframe)    # putting DTs in sites
```

```{r get_data, cache=TRUE}
source("C:/sleeper_fantasy/R/global.R")
 
# Get league info for 2019 season
s19_info <- F_get_lg_info(user_id = user_id, season = 2019)

# unload some information from this
lg_id <- s19_info$lg_id
lg_owners <- s19_info$user_vw0
lg_rules <- s19_info$lg_rules

#### Combining Data ####========================================================

# pull matchup and stat information
mu_out <- F_get_weekly_matchup(season = 2019, week = 1:3)
mu_view0 <- mu_out$mu_view0

stat_out <- F_get_weekly_player_stats(season = 2019, week = 1:3)
stat_sum0 <- stat_out$player_stats_sum0
stat_melt0 <- stat_out$stats_melt1

# combine the matchup and stat data
mu_view1 <- mu_view0 %>% 
  left_join(stat_sum0, 
            by = c("player_id" = "player_id", 
                   "week" = "week"))

# add in the owner name
mu_view2 <- mu_view1 %>% 
  left_join(lg_owners, 
            by = c("roster_id" = "roster_id"))

# add in player information
mu_view3 <- mu_view2 %>% 
  left_join(player_view0, 
            by = c("player_id" = "player_id"))
```

## Overview

This post will cover material analyzed in a prior weekly update. In the first week of the 2019 NFL Season, some of the results I came across motivated me to look into historical margin of victory. Once I had all that data loaded to my workspace, I also thought it would be fun(?) to check out Pythagorean Win Expectation. Since the 2019 season analyses are not going to be separate, weekly posts but rather a running post that changes every week, I am migrating those two side projects to this page. 

## Margin of Victory Analysis

Like I said, this came about because I noticed that the margin of victory for most matchups in the first week of the 2019 season were very high. 

```{r}

weekly_owner_scores0 <- mu_view3 %>% 
  group_by(week, display_name, starter, matchup_id) %>% 
  summarise(tot_points = sum(player_score, na.rm = TRUE)) %>% 
  ungroup()

# join this back onto 
weekly_owner_scores1 <- weekly_owner_scores0 %>% 
  left_join(weekly_owner_scores0, by = c("week", "matchup_id", "starter")) %>% 
  filter(display_name.x != display_name.y) %>% 
  mutate(Week = as.factor(week),
         display_name.x = as.factor(display_name.x)) %>% 
  select(Week, display_name.x, tot_points.x, tot_points.y, starter, matchup_id) %>% 
  rename(`Owner` = display_name.x, 
         `Points For` = tot_points.x, 
         `Points Against` = tot_points.y) %>% 
  arrange(Week) %>% 
  mutate(Outcome = case_when(`Points For` > `Points Against` ~ "Win", 
                             TRUE ~ "Loss")) %>% 
  group_by(Week, starter) %>% 
  mutate(`Week Median` = median(`Points For`), 
         `Week Mean` = mean(`Points For`)) %>% 
  ungroup()

mov0 <- weekly_owner_scores1 %>% 
  filter(Outcome == "Win", starter == TRUE) %>% 
  mutate(Margin = `Points For` - `Points Against`, 
         `Margin %` = round(Margin / `Points For`, 2))

ggplot(data = mov0, aes(x = Owner, y = Margin)) +
  geom_segment(aes(xend = Owner, yend = 0)) +
  geom_point(size = 2, colour = "black") +
  geom_text(aes(label = percent(`Margin %`), 
                y = Margin), 
            size = 3, colour = "#377EB8", fontface = "bold") +
  facet_wrap(~Week, scales = "free_y") + 
  scale_fill_gradient(low = "white", high = "#377EB8", label = percent, 
                      breaks = seq(0, 0.5, by = 0.25),
                      limits = c(0, 0.5)) +
  coord_flip() +
  ggtitle("TLST: TS - 2019 Season Margin of Victory",
          subtitle = "Margin % indicated in text computed as Margin / Points For") +
  labs(caption = "Data pulled through Sleeper API https://docs.sleeper.app/") +
   theme_fivethirtyeight() + 
  theme(axis.title = element_text(),
        axis.title.x =element_blank(),
        legend.title = element_blank()) +
  guides(fill = guide_legend(nrow = 1, keyheight = 0.5))

```

From this we see that margin of victory continues to be pretty high with a couple of real nail-biters sprinkled in here and there. A view of this for all seasons (Season 2019 through three week) can be seen below. 

```{r}

# Pull 2017, 2018 matchup information
# Need some league info first
s17_info <- F_get_lg_info(user_id = user_id, season = 2017)
s18_info <- F_get_lg_info(user_id = user_id, season = 2018)

# unload some information from this
lg_id_17 <- s17_info$lg_id
lg_id_18 <- s18_info$lg_id
# mu info

mu_17_out <- F_get_weekly_matchup(season = 2017, week = 1:16, .lg_id = lg_id_17)
mu_18_out <- F_get_weekly_matchup(season = 2018, week = 1:16, .lg_id = lg_id_18)

mu_17_view0 <- mu_17_out$mu_view0 %>%
  mutate(season = "2017")

mu_18_view0 <- mu_18_out$mu_view0 %>%
  mutate(season = "2018")

# Get only the topline numbers; join on to get MOV
mu_17_sum0 <- mu_17_view0 %>%
  filter(!is.na(matchup_id), starter == TRUE) %>%
  select(-player_id, -starter) %>%
  distinct()

mu_17_sum1 <- mu_17_sum0 %>%
  left_join(mu_17_sum0, by = c("matchup_id", "week", "season")) %>%
  filter(roster_id.x != roster_id.y)

 mu_17_sum1 <- mu_17_sum1 %>%
   mutate(outcome = case_when(points.x > points.y ~ "Win",
                              TRUE ~ "Loss")) %>%
   #select(-roster_id.x, -roster_id.y) %>%
   filter(outcome == "Win") %>%
   mutate(Margin = points.x - points.y)

 mu_18_sum0 <- mu_18_view0 %>%
   filter(!is.na(matchup_id), starter == TRUE) %>%
   select(-player_id, -starter) %>%
   distinct()

 mu_18_sum1 <- mu_18_sum0 %>%
   left_join(mu_18_sum0, by = c("matchup_id", "week", "season")) %>%
   filter(roster_id.x != roster_id.y) %>%
   mutate(outcome = case_when(points.x > points.y ~ "Win",
                              TRUE ~ "Loss")) %>%
   #select(-roster_id.x, -roster_id.y) %>%
   filter(outcome == "Win") %>%
   mutate(Margin = points.x - points.y)

# We have a lot of what we need in the mov0 object. Just do some re-naming
 mov1 <- mov0 %>% 
   mutate(season = "2019", 
          week = (as.character(Week)))
 
 moe_sum0 <- bind_rows(select(mu_17_sum1, week, season, Margin),
                       select(mu_18_sum1, week, season, Margin),
                       select(mov1, week, season, Margin))

 ggplot(data = moe_sum0, aes(x = as.factor(as.numeric(week)),
                            y = Margin, fill = season)) +
  geom_boxplot() +
  scale_x_discrete(name = "Week") +
  scale_fill_brewer(name = "Season",
                    palette = "Set1") +
  ggtitle("TLST: TS - Margin of Victory Distribution by Week",
          subtitle = "Solid black line in box is median; box is bound by 25th and 75th percentiles") +
  labs(caption = "Data pulled through Sleeper API https://docs.sleeper.app/") +
   theme_fivethirtyeight() + 
  theme(axis.title = element_text(),
        axis.title.x =element_blank(),
        legend.title = element_blank()) +
  guides(fill = guide_legend(nrow = 1, keyheight = 0.5))

 
```

We clearly see that the large margins of victory are common and have always been. 

## Pythagorean Win Expectation

Like I said, I have all the data in my `R` workspace to do Pythagorean analysis so we might as well do it. This part is unlikely to help anyone actually get better at fantasy. 

### What is This? 

The [Pythagorean Win Expectation](https://www.baseball-reference.com/bullpen/Pythagorean_Theorem_of_Baseball) was developed by Bill James for baseball and attempts to explain the relationship between runs and wins. The idea is that you can look at runs scored and runs allowed in a season and predict that team's winning percentage. Run differential is a simpler, related metrics but the Pythag approach does a better job. The initial formula for baseball is:

$$ \text{Winning Percentage} = \frac{\text{Runs Scored}^2}{\text{Runs Scored}^2 + \text{Runs Allowed}^2} $$

Let's compare this prediction for winning percentage to the simple run differential. We'll use data from the [Lahman](http://www.seanlahman.com/) database. He has all kinds of stuff going all the way back to the 1871 season, but we'll limit things from 1950 - 2018 to eliminate noise from The Drinkin' Beers and Rippin' Cigs while Playing Era. 

```{r, fig.align='center'}
library(Lahman)

teams_df <- Teams %>% 
  as_tibble() %>% 
  filter(yearID >= 1950) %>% 
  mutate(pythag = R ^ 2 / (R ^2 + RA ^2), 
         run_diff = R - RA,
         wp = W / G)

ggplot(data = teams_df, aes(x = pythag, y = wp)) + 
  geom_point() + 
  scale_x_continuous(name = "Pythagorean Estimate",
                     label = percent) + 
  scale_y_continuous(name = "Actual Winning Percentage", 
                     label = percent) + 
  geom_smooth(method = "lm", formula = y~x) + 
  geom_abline(slope = 1, intercept = 0, colour = "#E41A1C") + 
  ggtitle("Comparison of Pythagorean Estimate and\nActual MLB Winning Percentage",
          subtitle = "Red line indicates perfect fit. Actual fit is indicated in blue") + 
  labs(caption = "Lahman Data: 1950-2018") + 
theme_fivethirtyeight() + 
  theme(axis.title = element_text()) +
  guides(fill = guide_legend(nrow = 1, keyheight = 0.5))



```

Ok, well there you have it. It does a pretty good job. Nerds like to play with things and often find that an exponent of around 1.8 does a better job. Finding new exponents based on actual performance is important and do-able. Keep that in mind...

### Adapting for Football

It is possible to adapt this approach to NFL games. We can use points scored and points allowed. However, the [exponent for football is 2.37.](https://en.wikipedia.org/wiki/Pythagorean_expectation) Lets see how this works for our league. 

```{r}
# get owner info
owners_17_view0 <- s17_info$user_vw0
owners_18_view0 <- s18_info$user_vw0

mu_17_sum2 <- mu_17_sum0 %>%
  filter(!is.na(matchup_id)) %>% 
  left_join(mu_17_sum0, by = c("matchup_id", "week", "season")) %>%
  filter(roster_id.x != roster_id.y) %>% 
  left_join(owners_17_view0, by = c("roster_id.x" = "roster_id")) %>% 
  group_by(display_name, season) %>% 
   summarise(`Points For` = round(sum(points.x), 2), 
            `Points Against` = round(sum(points.y), 2), 
            Wins = sum(points.x > points.y, na.rm = TRUE), 
            Losses = sum(points.x < points.y, na.rm = TRUE), 
            win_p = round(sum(points.x > points.y) / n(), 2)) %>% 
  ungroup() %>% 
  mutate(pythag = round(`Points For`^2 / (`Points For` ^ 2 + `Points Against`^2), 2))

mu_18_sum2 <- mu_18_sum0 %>%
  filter(!is.na(matchup_id)) %>% 
  left_join(mu_18_sum0, by = c("matchup_id", "week", "season")) %>%
  filter(roster_id.x != roster_id.y) %>% 
  left_join(owners_18_view0, by = c("roster_id.x" = "roster_id")) %>% 
  group_by(display_name, season) %>% 
  summarise(`Points For` = round(sum(points.x), 2), 
            `Points Against` = round(sum(points.y), 2), 
            Wins = sum(points.x > points.y, na.rm = TRUE), 
            Losses = sum(points.x < points.y, na.rm = TRUE), 
            win_p = round(sum(points.x > points.y) / n(), 2)) %>% 
  ungroup() %>% 
  mutate(pythag = round(`Points For`^2.37 / (`Points For` ^ 2.37 + `Points Against`^2.37), 2))


# Get the information for 2019
mu_19_view0 <- mu_view0 %>%
  mutate(season = "2019")

# Get only the topline numbers; join on to get MOV
mu_19_sum0 <- mu_19_view0 %>%
  filter(!is.na(matchup_id), starter == TRUE) %>%
  select(-player_id, -starter) %>%
  distinct()

mu_19_sum1 <- mu_19_sum0 %>%
  left_join(mu_19_sum0, by = c("matchup_id", "week", "season")) %>%
  filter(roster_id.x != roster_id.y)

 mu_19_sum1 <- mu_19_sum1 %>%
   mutate(outcome = case_when(points.x > points.y ~ "Win",
                              TRUE ~ "Loss")) %>%
   #select(-roster_id.x, -roster_id.y) %>%
   filter(outcome == "Win") %>%
   mutate(Margin = points.x - points.y)

 mu_19_sum2 <- mu_19_sum0 %>%
  filter(!is.na(matchup_id)) %>% 
  left_join(mu_19_sum0, by = c("matchup_id", "week", "season")) %>%
  filter(roster_id.x != roster_id.y) %>% 
  left_join(lg_owners, by = c("roster_id.x" = "roster_id")) %>% 
  group_by(display_name, season) %>% 
  summarise(`Points For` = round(sum(points.x), 2), 
            `Points Against` = round(sum(points.y), 2), 
            Wins = sum(points.x > points.y, na.rm = TRUE), 
            Losses = sum(points.x < points.y, na.rm = TRUE), 
            win_p = round(sum(points.x > points.y) / n(), 2)) %>% 
  ungroup() %>% 
  mutate(pythag = round(`Points For`^2.37 / (`Points For` ^ 2.37 + `Points Against`^2.37), 2))


# combine these
pythag_sum <- mu_18_sum2 %>% bind_rows(mu_17_sum2) %>% bind_rows(mu_19_sum2) %>% 
  mutate(Difference = win_p - pythag) %>% 
  mutate_at(c("display_name", "season"), as.factor) %>% 
  rename(Owner= display_name, 
         `Win %` = win_p, 
         `Pythag Est` = pythag,
         Season = season) 
  

datatable(pythag_sum,
          class = 'cell-border stripe',
          rownames = FALSE, 
          caption = "TLST: TS Actual Winning % v. Pythagorean Expectation",
          filter = 'top',
          extensions = 'Buttons', 
          options = list(dom = 'lBfrtip', 
                         buttons = list('excel', "csv"),
                         pageLength = 10, autoWidth = TRUE))


```

There you have it as a table. Let's add a little color...

```{r, fig.align='center', fig.height=8, fig.width=8}

# melt this down
pythag_melt <- pythag_sum %>% 
  select(Season, Owner, `Win %`, `Pythag Est`) %>% 
  gather(key, value, -Season, -Owner)

ggplot(data = pythag_melt, aes(x = Owner, y = value, fill = key)) + 
  geom_col(position = "dodge", colour = "black") + 
  facet_wrap(~Season, scales = "free_y") + 
 scale_fill_brewer(palette = "Set1") + 
  scale_y_continuous(label = percent) + 
  coord_flip() + 
  ggtitle("TLST: TS - Win % v. Pythagorean Estimate",
          subtitle = "Pythagorean exponent of 2.37 used") + 
  labs(caption = "Data pulled through Sleeper API https://docs.sleeper.app/") +
  theme_fivethirtyeight() + 
  theme(axis.title = element_text(),
        axis.title.x =element_blank(),
        legend.title = element_blank()) +
  guides(fill = guide_legend(nrow = 1, keyheight = 0.5))

        


```

Well that is interesting. In addition to committing the mortal sin of Changing His Name and Ruining Data Integrity between Seasons, Charlie's actual winning percentages exceed what we'd expect based on the points he scores and the points scored against him in both seasons. Zach and Joe are guilty of this in 2017 as well. Sam suffers some really bad luck as his Pythagorean estimate is actually sort of respectable in 2017-not so much is actual percentage. And when you consider that these are percentages of 16 game seasons, something like my difference of 20% in 2018 is *three games* worth of difference. Charlie, feel free to drop that trophy off next time you're driving by Pocahontas. 

### Empirical Exponent

One that that we *can do* is determine a better fit for that exponent. If we say that $w$ is our winning percentage, we can set up a formula where the Pythagorean exponent is a variable. Typing is hard so we'll say that $\text{Points For} = PF$ and $\text{Points Against} = PA$. We'll end up with something like this: 


$$ 
w = \frac{W}{W + L}  = \frac{PF^n}{PF^n + PA^n}
$$

All we have to do now is switch this up a bit so that $n$ in linear with our other variables. Then we can use the known values of $PF$ and $PA$ for all of our teams over the last two seasons to model out what $n$ is. Solving for this is a simple matter of...

$\begin{align}

\begin{split}

w = \frac{W}{W + L}  = \frac{PF^n}{PF^n + PA^n}  \Rightarrow \text{ multiply each side by denominator} \\

  W \times (PF^n + PA^n) = PF^n \times (W + L)  \Rightarrow \text{ distribute} \\

  W \times PF^n + W \times PA^n = PF^n \times W + PF^n \times L  \Rightarrow \text{ subtract like terms from each side}  \\

  W \times PF^n + W \times PA^n = PF^n \times W + PF^n \times L  \Rightarrow  \text{ rearrange terms} \\

  W - L = PF^n - PA^n  \Rightarrow \text{ take natural log of each side} \\

  \log(W - L) = \log(PF^n - PA^n) \Rightarrow \text{ use properties of logs}  \\

  \log\frac{W}{L} = n\times \log \frac{PF}{PA}

  \end{split}

\end{align}$

And just like that, we now have an equation that is linear in $n$, our empirically determined exponent. This is good because now we can make a plot with $\ln\frac{W}{L}$ on the $y$ and $\ln\frac{\text{Points For}}{\text{Points Against}} $ on the $x$, draw a line through it and that slope is our actual exponent. 

```{r}

pythag_sum1 <- pythag_sum %>% 
  mutate(lgwl = log(Wins / Losses), 
         lgpts = log(`Points For` / `Points Against`))

lm1 <- lm(lgwl ~ lgpts + 0, data = pythag_sum1 %>% filter(Season != "2019"))

ggplot(data = pythag_sum1 %>% filter(Season != "2019"), aes(x = lgpts, y = lgwl)) + 
  geom_point() + 
  geom_smooth(method = "lm", formula = y~x + 0) + 
  scale_y_continuous(name = expression(log(frac("Wins", "Losses")))) + 
  scale_x_continuous(name = expression(log(frac("Points For", "Points Against")))) + 
  ggtitle("Log Ratio Relationship", 
          subtitle = "The slope of this graph represents the empirical Pythagorean exponent") +
  labs(caption = "Data pulled through Sleeper API https://docs.sleeper.app/") +
  theme_fivethirtyeight() + 
  theme(axis.title = element_text()) +
  guides(fill = guide_legend(nrow = 1, keyheight = 0.5))
  
  
```

Please note, we are forcing that line through 0 as the intercept. Oh also, since we are taking $\frac{\text{Wins}}{\text{Losses}}$ and then getting the $\text{log}$ of that for the line fit, the 2019 season is screwing everything up with divide by 0 errors so the model doesn't use that data. Whats that? You quit reading several sections ago? Fantastic. Anyways, I am about 20 minutes over budget on the time I was going to bury into this today and so I'm just going to bring the regression summary which gives us the slope of the line as the coefficient on `lgwl`. 


```{r}
pander::pander(summary(lm1)) 

```

So the coefficient for this league is actually 6.924. Crazy. 
